{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWy4KPll-3X1",
        "outputId": "e4527e2e-8379-4f81-ca6c-7af732ec8558"
      },
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import csv\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split      \n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer                    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ-rPxFJwuY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7af70b0-5203-4880-95e8-2668ca7268dd"
      },
      "source": [
        "data_X = {\n",
        "  'ES2002a' : open('ES2002a.transcript.txt', 'r'),\n",
        "  'ES2002b' : open('ES2002b.transcript.txt', 'r'),\n",
        "  'ES2002c' : open('ES2002c.transcript.txt', 'r'),\n",
        "  'ES2002d' : open('ES2002d.transcript.txt', 'r'),\n",
        "}\n",
        "\n",
        "data_Y = {\n",
        "  'ES2002a' : open('ES2002a.extsumm.txt', 'r'),\n",
        "  'ES2002b' : open('ES2002b.extsumm.txt', 'r'),\n",
        "  'ES2002c' : open('ES2002c.extsumm.txt', 'r'),\n",
        "  'ES2002d' : open('ES2002d.extsumm.txt', 'r'),\n",
        "}\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "sb_stemmer = SnowballStemmer('english')\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "filter_word = nltk_stopwords\n",
        "custom_stopwords = ['hmm', 'um', 'uh-huh', 'okay', 'uh', 'yeah', 'mm-hmm', 'uhm']\n",
        "filter_word = nltk_stopwords + custom_stopwords + list(string.punctuation)\n",
        "\n",
        "# Lemmatize the word, \n",
        "def stem(sentence):\n",
        "  tokens = word_tokenize(sentence)\n",
        "  stemmed_sentence = \"\"\n",
        "  for word in tokens:\n",
        "    if word not in filter_word:\n",
        "      stemmed_sentence = stemmed_sentence + \" \" + lemmatizer.lemmatize(word.lower())\n",
        "      \n",
        "  \n",
        "  return stemmed_sentence\n",
        "\n",
        "# check for each label is present in X\n",
        "def compare(x_token, y_token):\n",
        "  y_token_copy = y_token\n",
        "  count = len(y_token)\n",
        "\n",
        "  for x in x_token:\n",
        "    if x in y_token:\n",
        "      y_token_copy.remove(x)\n",
        "  \n",
        "  count_copy = len(y_token_copy)\n",
        "  if (count - count_copy) >1:\n",
        "    return True, y_token_copy\n",
        "  else: \n",
        "    return False, y_token\n",
        "\n",
        "total_tokens = 0\n",
        "total_tokenized = 0\n",
        "for document in data_X:\n",
        "  document_x = data_X[document].read()\n",
        "  document_y = data_Y[document].read()\n",
        "  sentence_x = nltk.sent_tokenize(document_x)\n",
        "  sentence_y = nltk.sent_tokenize(document_y)\n",
        "  \n",
        "  print(\"Document: {}, X: {}, Y: {}\".format(document,len(sentence_x),len(sentence_y)))\n",
        "  y_count = 0\n",
        "  for index_x,value_x in enumerate(sentence_x):\n",
        "    x_token = word_tokenize(value_x)\n",
        "    y_token = word_tokenize(sentence_y[y_count])\n",
        "    is_decision, y_token = compare(x_token,y_token)\n",
        "    if is_decision:\n",
        "      Y.append(1)\n",
        "    else:\n",
        "      Y.append(0)\n",
        "    if not y_token:\n",
        "      y_count += 1\n",
        "    X.append(stem(value_x))\n",
        "    total_tokens += len(word_tokenize(value_x))\n",
        "    total_tokenized += len(word_tokenize(stem(value_x)))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document: ES2002a, X: 332, Y: 41\n",
            "Document: ES2002b, X: 691, Y: 143\n",
            "Document: ES2002c, X: 633, Y: 94\n",
            "Document: ES2002d, X: 854, Y: 107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4bG_gHWp92H"
      },
      "source": [
        "#split into 75% training data and 25% testing data\n",
        "x_train,x_test,y_train,y_test = train_test_split(X, Y, random_state=1000, stratify= Y, test_size=0.25, )\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(x_train)\n",
        "X_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Adding 1 because of  reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1                          \n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "#embed words into GloVe embedding\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "  vocab_size = len(word_index) + 1  \n",
        "  # Adding again 1 because of reserved 0 index\n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word in word_index:\n",
        "        idx = word_index[word] \n",
        "        embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "  return embedding_matrix\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt' , tokenizer.word_index, embedding_dim)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HANDQ1XmRh0w"
      },
      "source": [
        "Train the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVJvpFgsRg0D",
        "outputId": "ca52f628-636d-43c9-f6f3-72a665866d1a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\n",
        "  f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "  return f1_val\n",
        "\n",
        "kernel_size = [3,5,20]\n",
        "num_fil = [4,8,64,128]\n",
        "\n",
        "for f in num_fil:\n",
        "  for k in kernel_size:\n",
        "    f1 = 0\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    for i in range(10):\n",
        "      model = Sequential()\n",
        "      model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "      model.add(layers.Conv1D(f, k, activation='relu'))\n",
        "      model.add(layers.GlobalMaxPooling1D())\n",
        "      model.add(layers.Dense(10, activation='relu'))\n",
        "      model.add(layers.Dense(1, activation='sigmoid'))\n",
        "      model.compile(optimizer='adam', loss='binary_crossentropy', metrics= [tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), get_f1])\n",
        "      history = model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=0)\n",
        "      result = model.evaluate(X_test, y_test, verbose=0)\n",
        "      precision += result[1]\n",
        "      recall += result[2]\n",
        "      f1 += result[3]\n",
        "\n",
        "    print(\"Number of filter: \", f, \", Kernel size: \", k)\n",
        "    print(\"Precision: \",precision/10, \"Recall: \", recall/10 ,\"F1: \",f1/10)  \n",
        "\n",
        "#print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of filter:  4 , Kernel size:  3\n",
            "Precision:  0.8082627534866333 Recall:  0.760638302564621 F1:  0.7663241446018219\n",
            "Number of filter:  4 , Kernel size:  5\n",
            "Precision:  0.8040926575660705 Recall:  0.758156031370163 F1:  0.7650553762912751\n",
            "Number of filter:  4 , Kernel size:  20\n",
            "Precision:  0.8066249251365661 Recall:  0.7585106432437897 F1:  0.7651508748531342\n",
            "Number of filter:  8 , Kernel size:  3\n",
            "Precision:  0.8145659565925598 Recall:  0.7499999940395355 F1:  0.7653832733631134\n",
            "Number of filter:  8 , Kernel size:  5\n",
            "Precision:  0.8086940050125122 Recall:  0.7482269465923309 F1:  0.7613737165927887\n",
            "Number of filter:  8 , Kernel size:  20\n",
            "Precision:  0.8004379391670227 Recall:  0.7641843974590301 F1:  0.7660239279270172\n",
            "Number of filter:  64 , Kernel size:  3\n",
            "Precision:  0.8057088673114776 Recall:  0.7496453881263733 F1:  0.7623980522155762\n",
            "Number of filter:  64 , Kernel size:  5\n",
            "Precision:  0.7957610845565796 Recall:  0.7695035457611084 F1:  0.7674879789352417\n",
            "Number of filter:  64 , Kernel size:  20\n",
            "Precision:  0.8074239552021026 Recall:  0.7382978677749634 F1:  0.7554419219493866\n",
            "Number of filter:  128 , Kernel size:  3\n",
            "Precision:  0.8012786686420441 Recall:  0.7609929084777832 F1:  0.7666896402835846\n",
            "Number of filter:  128 , Kernel size:  5\n",
            "Precision:  0.7971299707889556 Recall:  0.7570922017097473 F1:  0.7600287914276123\n",
            "Number of filter:  128 , Kernel size:  20\n",
            "Precision:  0.8064454436302185 Recall:  0.7471631228923797 F1:  0.758045893907547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhvEngMpnf52"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}